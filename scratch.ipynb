{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51da30-c2ed-4ae5-bb39-5aa692c91bd0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, datediff, lit\n",
    "from pyspark.sql.types import *\n",
    "from os import walk\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from Helper_Code.helpfull_functions import *\n",
    "from Helper_Code.helper_variables import *\n",
    "from Helper_Code.quality_checks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a63ad0-bb9f-4794-baab-f290699c03e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_spark_session is a helper function that will load the session with the appropriate configs\n",
    "spark = get_spark_session(\"test\", SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220444c-0974-416f-8391-7f3dd850cef4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge_files is a potentially obsolete function that groups all the parquet files in a folder, can select for showing the resulting dataframe or not\n",
    "concept = merge_files(\"/home/jupyter/omop-ed-datapipeline/concept\", spark, show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee99e018-40c5-432b-a8c6-f664f9f5d5de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loads visit occurences\n",
    "visit_occurrence = merge_files(\"/home/jupyter/omop-ed-datapipeline/visit_occurrence\", spark, show = False)\n",
    "\n",
    "#Selects for only the columns related to the times of the visit\n",
    "visit_length_columns = ['person_id', 'visit_occurrence_id', 'visit_start_datetime', 'visit_end_datetime']\n",
    "visit_length = visit_occurrence.select([col for col in visit_length_columns])\n",
    "\n",
    "#Creates a new column of the length of the visits in seconds\n",
    "visit_length=visit_length.withColumn('visit_length',col(\"visit_end_datetime\").cast(\"long\") - col('visit_start_datetime').cast(\"long\"))\n",
    "visit_length.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92396a42-9849-4fbc-afb3-832dbd10b49f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "payer_plan_period = merge_files(\"/home/jupyter/co-morbidity-omop/payer_plan_period\", spark, show= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6e0c0-2049-4c1e-92d9-cd20dcdefabf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Person file is mostly demographic info\n",
    "person = merge_files(\"/home/jupyter/omop-ed-datapipeline/person\", spark, show = False)\n",
    "\n",
    "#Selects the columns that we want\n",
    "demographics_cols = ['person_id', 'birth_datetime', 'death_datetime', 'gender_source_value', 'race_source_value', 'ethnicity_source_value']\n",
    "demo = person.select([col for col in demographics_cols])\n",
    "\n",
    "#In order to get patient age we need to compair birth date and visit time so we first merge with visit information. Can alternatively merge on visit level instead of person level\n",
    "demo = demo.join(visit_length, on = \"person_id\", how = \"outer\")\n",
    "\n",
    "#Create a new age column in years\n",
    "demo = demo.withColumn(\"age\", datediff(col(\"visit_start_datetime\"),col(\"birth_datetime\"))/365.25)\n",
    "\n",
    "#Gets rid of visit level so only demographics are left\n",
    "demo = demo.select([col for col in ['person_id', 'birth_datetime', 'death_datetime', 'gender_source_value', 'race_source_value', 'ethnicity_source_value', 'age']])\n",
    "\n",
    "#Selects for over 65\n",
    "demo = demo.where((demo.age >= 65))\n",
    "\n",
    "#Gets only unique rows as there will be multiple encounters per patient and when you get rid of the visit level columns they are identical.\n",
    "demo = demo.distinct()\n",
    "demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35cf506-fb25-45e5-91df-51f5525bb27b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save in a csv\n",
    "\n",
    "#I think it is possible to save in one CSV instead of chunks. Pyspark is set up where each dataframe is chunked out so they can be passed to parallel cores for more efficient work.\n",
    "#I haven't looked into it but I think theres a way to manualey specify the number of these chunks. I believe it saves a csv per chunk so you can manually set it to one chunk right\n",
    "#before saving and it should save as one CSV\n",
    "#... maybe\n",
    "demo.write.option(\"header\",True).csv(\"democsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d837fcc-03b8-4357-bc04-a3a3cd8ae5e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Condition file contains all the diagnoses a patient might have in the form of ICD-9/10, or SNOWMED codes. \n",
    "condition_occurrence = merge_files(\"/home/jupyter/omop-ed-datapipeline/condition_occurrence\", spark, show = False)\n",
    "\n",
    "#Select columns\n",
    "condition_columns = ['person_id', 'condition_occurrence_id', 'condition_occurrence_source_id', 'condition_concept_id', 'condition_start_datetime', 'condition_end_datetime', 'condition_type_concept_id', 'condition_status_concept_id', 'visit_occurrence_id', 'condition_source_value', 'condition_source_concept_id']\n",
    "conditions = condition_occurrence.select([col for col in condition_columns])\n",
    "\n",
    "#Merge the concepts dataframe with the conditions to get more information about the condition_concept_id\n",
    "#And column with the concept_id at the end preceded by something can be compared to the concept.concept_id column\n",
    "conditions = conditions.join(concept, conditions.condition_concept_id == concept.concept_id)\n",
    "\n",
    "#Get only the columns we want\n",
    "merged_columns = ['person_id', 'condition_concept_id', 'condition_start_datetime', 'condition_end_datetime', 'visit_occurrence_id', 'condition_source_value', 'concept_name', 'domain_id', 'vocabulary_id', 'concept_class_id', 'concept_code']\n",
    "conditions = conditions.select([col for col in merged_columns])\n",
    "\n",
    "#Format the visit length to what we currently need and merge it on visit level with the conditions\n",
    "visit_length_columns = ['visit_occurrence_id', 'visit_start_datetime', 'visit_end_datetime']\n",
    "visit_length2 = visit_length.select([col for col in visit_length_columns])\n",
    "conditions = conditions.join(visit_length2, on = \"visit_occurrence_id\", how = \"outer\")\n",
    "\n",
    "#Selects the conditions that started before the visit startdate and haven't ended by the time the visit started. This is in affect the current conditions of the patient\n",
    "conditions = conditions.where((conditions.condition_start_datetime < conditions.visit_start_datetime) & (conditions.condition_end_datetime > conditions.visit_start_datetime))\n",
    "\n",
    "#Trim the columns\n",
    "merged_columns = ['person_id', 'visit_occurrence_id', 'condition_concept_id', 'condition_source_value', 'concept_name', 'domain_id', 'vocabulary_id', 'concept_class_id', 'concept_code']\n",
    "conditions = conditions.select([col for col in merged_columns])\n",
    "\n",
    "#This groups the conditions by visit. So instead of one row per condition its one row per encounter. I don't know why its called \"F\" but for some reason thats an important thing.\n",
    "# There are various F.collect_XXXX functions. collect_list groups all the items into a list, but there are ones for average, mean, sum, and things like that too\n",
    "conditions = conditions.groupby(\"visit_occurrence_id\", \"person_id\").agg(F.collect_list(\"condition_concept_id\"), F.collect_list(\"condition_source_value\"), F.collect_list(\"concept_name\"), F.collect_list(\"domain_id\"), F.collect_list(\"vocabulary_id\"), F.collect_list(\"concept_class_id\"), F.collect_list(\"concept_code\"))\n",
    "\n",
    "#Renames the columns back to a understanable name cause its annoying to have all the columns called collect_list(XXXX)\n",
    "conditions = conditions.withColumnRenamed(\"collect_list(condition_concept_id)\",\"condition_concept_id\")\\\n",
    "                                .withColumnRenamed(\"collect_list(condition_source_value)\",\"condition_source_value\")\\\n",
    "                                .withColumnRenamed(\"collect_list(concept_name)\",\"concept_name\")\\\n",
    "                                .withColumnRenamed(\"collect_list(domain_id)\",\"domain_id\")\\\n",
    "                                .withColumnRenamed(\"collect_list(vocabulary_id)\",\"vocabulary_id\")\\\n",
    "                                .withColumnRenamed(\"collect_list(concept_class_id)\",\"concept_class_id\")\\\n",
    "                                .withColumnRenamed(\"collect_list(concept_code)\",\"concept_code\")\n",
    "# conditions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9e5bf5-6a96-457c-a4d0-dd2b6077bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is some hacky stuff.\n",
    "#CSVs cant save cells with list objects in them so for all the columns that contain lists we have to cast them to strings\n",
    "columns_to_string = ['condition_concept_id', 'condition_source_value', 'concept_name', 'domain_id', 'vocabulary_id', 'concept_class_id', 'concept_code']\n",
    "for c in columns_to_string:\n",
    "    conditions = conditions.withColumn(c, col(c).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc16800-446b-42a8-9a83-79fbd6d21167",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conditions.write.option(\"header\",True).csv(\"conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b92758-7700-4aec-b437-9b6298725571",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All this is super similar to conditions so im just gonna point out the differences\n",
    "\n",
    "procedure = merge_files(\"/home/jupyter/omop-ed-datapipeline/procedure_occurrence\", spark, show = False)\n",
    "\n",
    "pocedure_columns = ['person_id', 'procedure_concept_id', 'procedure_datetime', 'visit_occurrence_id', 'procedure_source_value']\n",
    "procedure_occurrence = procedure.select([col for col in pocedure_columns])\n",
    "\n",
    "procedure_occurrence = procedure_occurrence.join(concept, procedure_occurrence.procedure_concept_id == concept.concept_id)\n",
    "\n",
    "merged_columns = ['person_id', 'procedure_datetime', 'visit_occurrence_id', 'procedure_source_value', 'concept_name', 'vocabulary_id']\n",
    "procedure_occurrence = procedure_occurrence.select([col for col in merged_columns])\n",
    "\n",
    "procedure_occurrence = procedure_occurrence.join(visit_length, on = [\"visit_occurrence_id\", \"person_id\"], how = \"outer\")\n",
    "\n",
    "#Procedures are mostly denoted by CPT codes so here one of the columns is vocabulary_id which specifies which language the code is from (ICD, SNOMED, CPT, RxNORM etc)\n",
    "#And we select for CPT\n",
    "procedure_occurrence = procedure_occurrence.where((procedure_occurrence.vocabulary_id == 'CPT4'))\n",
    "\n",
    "merged_columns = ['person_id', 'visit_occurrence_id', 'procedure_datetime', 'procedure_source_value', 'concept_name', 'vocabulary_id']\n",
    "procedure_occurrence = procedure_occurrence.select([col for col in merged_columns])\n",
    "\n",
    "procedure_occurrence = procedure_occurrence.groupby(\"visit_occurrence_id\", \"person_id\").agg(F.collect_list(\"procedure_datetime\"), F.collect_list(\"procedure_source_value\"), F.collect_list(\"concept_name\"), F.collect_list(\"vocabulary_id\"))\n",
    "\n",
    "procedure_occurrence = procedure_occurrence.withColumnRenamed(\"collect_list(procedure_datetime)\",\"procedure_datetime\")\\\n",
    "                                .withColumnRenamed(\"collect_list(procedure_source_value)\",\"procedure_source_value\")\\\n",
    "                                .withColumnRenamed(\"collect_list(concept_name)\",\"concept_name_procedure\")\\\n",
    "                                .withColumnRenamed(\"collect_list(vocabulary_id)\",\"vocabulary_id_procedure\")\n",
    "# procedure_occurrence.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3ffbbf-4d0e-4a00-850f-c214e08c9f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_string = [ 'procedure_datetime', 'procedure_source_value', 'concept_name_procedure', 'vocabulary_id_procedure']\n",
    "for c in columns_to_string:\n",
    "    procedure_occurrence = procedure_occurrence.withColumn(c, col(c).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420ab8-76e1-4b25-9450-a68101406eae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "procedure_occurrence.write.option(\"header\",True).csv(\"procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08876e62-3c28-4eb5-88b3-0cb6d1b6e169",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Same idea as procedures and conditions\n",
    "\n",
    "drug_exposure = merge_files(\"/home/jupyter/omop-ed-datapipeline/drug_exposure\", spark, show = False)\n",
    "\n",
    "\n",
    "dug_columns = ['person_id', 'drug_concept_id', 'drug_exposure_start_datetime', 'drug_exposure_end_datetime', 'refills', 'quantity', 'route_concept_id', 'visit_occurrence_id', 'drug_generic_source_value_name', 'route_source_value']\n",
    "drugs = drug_exposure.select([col for col in dug_columns])\n",
    "\n",
    "drugs = drugs.join(concept, drugs.drug_concept_id == concept.concept_id)\n",
    "\n",
    "dug_columns = ['person_id', 'drug_concept_id', 'drug_exposure_start_datetime', 'drug_exposure_end_datetime', 'refills', 'quantity', 'route_concept_id', 'visit_occurrence_id', 'drug_generic_source_value_name', 'route_source_value', 'concept_name', 'vocabulary_id' ]\n",
    "drugs = drugs.select([col for col in dug_columns])\n",
    "\n",
    "drugs = drugs.groupby(\"visit_occurrence_id\", \"person_id\").agg(F.collect_list(\"drug_concept_id\"), F.collect_list(\"drug_exposure_start_datetime\"), F.collect_list(\"drug_exposure_end_datetime\"), F.collect_list(\"drug_generic_source_value_name\"), F.collect_list(\"vocabulary_id\"))\n",
    "\n",
    "drugs = drugs.withColumnRenamed(\"collect_list(drug_concept_id)\",\"drug_concept_id\")\\\n",
    "                                .withColumnRenamed(\"collect_list(drug_exposure_start_datetime)\",\"drug_exposure_start_datetime\")\\\n",
    "                                .withColumnRenamed(\"collect_list(drug_exposure_end_datetime)\",\"drug_exposure_end_datetime\")\\\n",
    "                                .withColumnRenamed(\"collect_list(drug_generic_source_value_name)\",\"drug_generic_source_value_name\")\\\n",
    "                                .withColumnRenamed(\"collect_list(vocabulary_id)\",\"vocabulary_id_drug\")\n",
    "# drugs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc1c2c-9c92-4165-b42a-e4e19d6d6f1f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The measurment file contains things like GCS, labs and stuff like that\n",
    "measurement = merge_files(\"/home/jupyter/omop-ed-datapipeline/measurement\", spark, show = False)\n",
    "\n",
    "measurement_columns = ['person_id', 'measurement_concept_id', 'measurement_datetime', 'order_datetime', 'measurement_time', 'measurement_type_concept_id', 'value_as_number', 'value_as_string', 'visit_occurrence_id', 'measurement_source_value', 'measurement_source_value_alt']\n",
    "measurement = measurement.select([col for col in measurement_columns])\n",
    "\n",
    "measurement = measurement.join(concept, measurement.measurement_concept_id == concept.concept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13266f-5447-4fe3-94f4-7b9bf1a02da5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This was my best bet for pulling triage and GCS data\n",
    "measurement2= measurement.where((concept.concept_name.contains('triage')) | (concept.concept_name.contains('GCS')))\n",
    "measurement2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af799998-5ec1-483e-935a-97b47312e5fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "measurement2.write.option(\"header\",True).csv(\"GCS_Traige\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6290bb-67fd-4b7a-8bd5-9187718a255c",
   "metadata": {},
   "source": [
    "This is all garbage scratch code of me trying to find trage data and not being able too as well as figuring out some other functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123fc985-a85d-40f7-acad-b04b39dc48c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "observation = merge_files(\"/home/jupyter/omop-ed-datapipeline/observation\", spark, show= False)\n",
    "observation_columns = ['person_id', 'observation_id', 'specimen_id', 'observation_concept_id', 'observation_datetime', 'observation_type_concept_id', 'value_as_number', 'value_as_string',  'visit_occurrence_id', 'observation_event_id', 'obs_event_field_concept_id']\n",
    "observation = observation.select([col for col in observation_columns])\n",
    "observation = observation.join(concept, observation.observation_concept_id == concept.concept_id)\n",
    "observation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf5c93-f5c9-4d3d-a750-1ab14365c0ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "observation2= observation.where((concept.concept_name.contains('triage')))\n",
    "observation2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c01108-5c7b-406b-ab65-adbbc3cbe0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation2.write.option(\"header\",True).csv(\"Traige\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c618e4-241e-40b1-a283-279e325b16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_string = ['drug_concept_id', 'drug_exposure_start_datetime', 'drug_exposure_end_datetime', 'drug_generic_source_value_name', 'vocabulary_id_drug']\n",
    "\n",
    "for c in columns_to_string:\n",
    "    drugs = drugs.withColumn(c, col(c).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da281bde-b9e5-405b-83e9-8c8e840cfd9b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "drugs.write.option(\"header\",True).csv(\"drugs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ef84d-f8ec-4f4b-a794-1a8881faf072",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "patients = demo.join(conditions, on='person_id', how= \"left\")\n",
    "patients2 = patients.join(procedure_occurrence, on=['visit_occurrence_id','person_id'], how= \"left\")\n",
    "patients3 = patients2.join(drugs, on=['visit_occurrence_id','person_id'], how= \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b26e89-af7e-422f-b453-177aa589f692",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_string = ['drug_concept_id', 'drug_exposure_start_datetime', 'drug_exposure_end_datetime', 'drug_generic_source_value_name', 'vocabulary_id_drug', 'procedure_datetime', 'procedure_source_value', 'concept_name_procedure', 'vocabulary_id_procedure', 'condition_concept_id', 'condition_source_value', 'concept_name', 'domain_id', 'vocabulary_id', 'concept_class_id', 'concept_code']\n",
    "\n",
    "for c in columns_to_string:\n",
    "    patients3 = patients3.withColumn(c, col(c).cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24d68c-3c68-4bdb-9e9b-71f3da92779f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# patients3.write.csv('patients_over_65.csv')\n",
    "patients3.write.option(\"header\",True).csv(\"/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec4b6b-a115-494a-838e-c88cab379209",
   "metadata": {},
   "source": [
    "Prototype function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0190f8-1531-4ac7-8229-9d485e39ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THis is a funciton I was working on related to identifying column types for quality checks\n",
    "\n",
    "def dataframe_column_types(df):\n",
    "    numerical_types = [LongType(), IntegerType(), FloatType(), DecimalType(), DoubleType(), ShortType()]\n",
    "    string_types = [StringType()]\n",
    "    null_types = [NullType()]\n",
    "    time_types = [TimestampType()]\n",
    "    num_cols = []\n",
    "    string_cols = []\n",
    "    null_cols = []\n",
    "    time_cols = []\n",
    "    for col in df.columns:\n",
    "        dtype_col = df.schema[col].dataType\n",
    "        if dtype_col in numerical_types:\n",
    "            print(col, dtype_col, \"numerical\")\n",
    "            num_cols.append(col)\n",
    "        if dtype_col in string_types:\n",
    "            print(col, dtype_col, \"string\")\n",
    "            string_cols.append(col)\n",
    "        if dtype_col in null_types:\n",
    "            print(col, dtype_col, \"null\")\n",
    "            null_cols.append(col)\n",
    "        if dtype_col in time_types:\n",
    "            print(col, dtype_col, \"time\")\n",
    "            time_cols.append(col)\n",
    "    return [num_cols, string_cols, null_cols, time_cols]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9f72f-4a9f-4d2a-8c9c-07f2ccceb7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc773ce-f7f1-4639-b28d-444a7dbb60a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
